{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATS Workshops Info Scraping Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The Idea\n",
    "\n",
    "\n",
    "**Three steps of web scraping**\n",
    "\n",
    "   - Making requests\n",
    "   - Connecting and dumping rescources(HTML/XML/JSON)\n",
    "   - Analyzing and extracting information\n",
    "\n",
    "When we visit a website, we want to get the web pages from the web server, so our web browser makes a request(a connection) to the server, telling it who we are and what files we are asking for. The server then sends back files that contain fragments that let our browser render a web page. \n",
    "Those send-back files can be in the forms of HTML, XML, JSON, IMG, CSS, etc. Most of the time, the text information, which is what we usually want, is conveyed in the forms of HTML, XML, and JSON.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Making requests\n",
    "\n",
    "We mainly use the package called `urllib.request` to make Http requests and use the requests to make connections. \n",
    "\n",
    "You can also use the package `requests` to do this.\n",
    "\n",
    "The way to use `urllib.request` to make a request is like this:\n",
    "\n",
    "```\n",
    "req = urllib.request.Request(\n",
    "        my_url, \n",
    "        data=None, \n",
    "        headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "Here is the documentation for [`urllib.request.Request()`](https://docs.python.org/3/library/urllib.request.html#urllib.request.Request)\n",
    "\n",
    "There are three parameters we should define: `url`, `data`, and `headers`.\n",
    "\n",
    "> * `url` should be a string containing a valid URL.\n",
    "\n",
    "> * `data` must be an object specifying additional data to send to the server, or None if no such data is needed. \n",
    "\n",
    "> * `headers` should be a dictionary, and will be treated as if add_header() was called with each key and value as arguments. This is often used to “spoof” the User-Agent header value, which is used by a browser to identify itself – some HTTP servers only allow requests coming from common browsers as opposed to scripts. For example, Mozilla Firefox may identify itself as \"Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11\", while urllib’s default user agent string is \"Python-urllib/2.6\" (on Python 2.6).\n",
    "\n",
    "There is another parameter that can be used to set methods as `GET`/`POST`. The default is `GET` if `data` is `None` or `POST` otherwise.\n",
    "\n",
    "\n",
    "**Tips**\n",
    "\n",
    "Some times, the request method may not be 'GET', especially when craping data from tables on web pages. At this time, the data is more likely transfered in the form of JSON or XML by the method of 'POST'. Use the development tool of your web broswer to figure out which request plays the role of data transfering and use the url of that request as your request url and make sure using the same request method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Connecting and dumping rescources\n",
    "\n",
    "Once made a requet, we can send it using `urllib.request.urlopen` to make connection:\n",
    "\n",
    "```\n",
    "uClient = urllib.request.urlopen(req)\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "```\n",
    "\n",
    "At the same time, we dump the rescources by calling the `read()` function and this function will return the data in string type even you've already known it is an html file. Therefore, you need to analyze it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Analyzing and extracting inforamtion\n",
    "\n",
    "If you're getting an html file, then you need parse it with an html.parser:\n",
    "\n",
    "```\n",
    "page_soup = soup(page_html, \"html.parser\")\n",
    "```\n",
    "\n",
    "We can call this parsing procedure as \"soup\". A soup object will be return.\n",
    "\n",
    "The next steps are info extraction. The methosdology is to extract info by walking through the DOM and searching its tags and variables of those tags. Functions used in these steps can be `findAll()` or `select()` of `BeautifulSoup` objects. In this program, I mainly used `findAll()`. Besides, buld-in string functions can be used in string processing. For deatils, check out the code below.\n",
    "\n",
    "\n",
    "**Tips**\n",
    "\n",
    "- To learn the sturcture of traget web page, use the development tool of your web broswer to inspect it.\n",
    "\n",
    "- To store the scraped data, you can save it directly in a csv file or create a `DataFrame` with `pandas` and manipulate the data with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.debug('This is a log message.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Customized reusable soup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_a_page(req):\n",
    "    # make connection\n",
    "    uClient = urllib.request.urlopen(req)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    \n",
    "    # html parser\n",
    "    page_soup = soup(page_html, \"html.parser\")\n",
    "    \n",
    "    return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_register_info(courseId):\n",
    "    logging.debug(\"soup_register: \" + str(courseId))\n",
    "    post_data = {'intCourseId':courseId}\n",
    "    return_data = requests.post(\"https://northeastern.gosignmeup.com/public/Course/CourseDetails\",data=post_data)\n",
    "    \n",
    "    return_soup = soup(return_data.content, \"html.parser\")\n",
    "    \n",
    "    return return_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Scrape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(my_url, f):\n",
    "    logging.debug('start scraping')\n",
    "    \n",
    "    # creat a request of fake user agent\n",
    "    req = urllib.request.Request(\n",
    "        my_url, \n",
    "        data=None, \n",
    "        headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # html parser\n",
    "    page_soup = soup_a_page(req)\n",
    "    \n",
    "    # grab all articles\n",
    "    articles = page_soup.findAll(\"article\")\n",
    "    \n",
    "    #grab title, date, and time\n",
    "    for article in articles:\n",
    "        \n",
    "        #title and title_date\n",
    "        raw_title = article.header.h2.a.text\n",
    "        title = raw_title.rpartition(\" \")[0]\n",
    "        title_date = raw_title.rpartition(\" \")[2]\n",
    "        \n",
    "        #subtitle_date, start_time, end_time\n",
    "        date_and_time = article.header.h5.text.strip().partition(\"\\xa0\\xa0•\\xa0\\n\")\n",
    "        subtitle_date = date_and_time[0].replace(\",\" , \"\").replace(\" \", \"/\").replace(\"May\", \"5\").replace(\"Jun\", \"6\").replace(\"Jul\", \"7\").replace(\"Aug\", \"8\")\n",
    "        time = date_and_time[2].strip()\n",
    "        start_time = time.partition(\" - \")[0]\n",
    "        end_time = time.partition(\" - \")[2]\n",
    "        \n",
    "        #details page\n",
    "        detail_url = article.div.findAll(\"p\")[1].findAll(href=True)[0]['href']\n",
    "        \n",
    "        #make new connection for detail page\n",
    "        detail_soup = soup_a_page(detail_url)\n",
    "        \n",
    "        #grab location\n",
    "        location = detail_soup.findAll(\"article\")[0].header.findAll(\"h5\")[2].text.partition(\": \")[2]\n",
    "        \n",
    "        #grab register link and soup it\n",
    "        register_url = detail_soup.findAll(\"article\")[0].section.findAll(\"a\")[0]['href']\n",
    "        courseId_str = register_url.rsplit(\"=\")[1]\n",
    "        courseId_int = int(courseId_str)\n",
    "        \n",
    "        return_soup = soup_register_info(courseId_int)\n",
    "        \n",
    "        #course name\n",
    "        reg_course_name = return_soup.div['data-course-name']\n",
    "        \n",
    "        #location\n",
    "        reg_location = return_soup.find(\"input\",{\"id\":\"hdlocation\"})['value']\n",
    "        \n",
    "        session_div = return_soup.find(\"div\",{\"id\":\"CourseDates_and_TimesContainerDet\"}).findAll(\"div\",{\"style\":\"padding:5px; width:100%; min-height:20px; overflow: auto;\"})[0]\n",
    "        session_info_div = session_div.findAll(\"div\", {\"style\": \"padding:5px; height:20px; display: table-row; width:100%;\"})[0]\n",
    "        \n",
    "        #date\n",
    "        reg_date = session_info_div.findAll(\"div\")[0].text.strip()\n",
    "        \n",
    "        #start time\n",
    "        reg_start_time = session_info_div.findAll(\"div\")[1].text.replace(\"\\xa0\",\"\").replace(\"\\r\\n\",\"\").replace(\"(EST)\",\"\").strip().partition(\" - \")[0]\n",
    "        \n",
    "        #end time\n",
    "        reg_end_time = session_info_div.findAll(\"div\")[1].text.replace(\"\\xa0\",\"\").replace(\"\\r\\n\",\"\").replace(\"(EST)\",\"\").strip().partition(\" - \")[2]\n",
    "        \n",
    "        instructors = []\n",
    "        \n",
    "        try:\n",
    "            instructors_h2s = return_soup.findAll(\"div\",{\"id\":\"CourseInstructorContainerDet\"})[0].findAll(\"h2\")\n",
    "        except:\n",
    "            instructors_h2s = return_soup.findAll(\"div\",{\"id\":\"CourseInstructorsContainerDet\"})[0].findAll(\"h2\")\n",
    "        \n",
    "        \n",
    "        for instructor in instructors_h2s:\n",
    "            name = instructor.b.text.replace(\"\\xa0\",\"\")\n",
    "            instructors.append(name)\n",
    "            logging.debug(\"appended instructor: \" + name)\n",
    "        \n",
    "        reg_instructors = \"\"    \n",
    "        \n",
    "        for i in instructors:\n",
    "            reg_instructors += i + \"|\"\n",
    "        \n",
    "        #instructors\n",
    "        reg_instructors = reg_instructors.rsplit(\"|\",1)[0]\n",
    "        \n",
    "        \n",
    "        #write info into csv\n",
    "        f.write(title + \",\" + title_date + \",\" + subtitle_date + \",\" + start_time + \",\" + end_time\n",
    "                + \",\" + location + \",\" + reg_course_name + \",\" + reg_date + \",\" + reg_start_time + \",\"\n",
    "                + reg_end_time + \",\" + reg_location + \",\" + reg_instructors + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # test page url\n",
    "    my_url = \"https://www.northeastern.edu/ats/event/\"\n",
    "    #initialize output csv file\n",
    "    fileName = \"workshops.csv\"\n",
    "    f = open(fileName, \"w\", encoding='utf-8')\n",
    "    logging.debug('opened file:' + fileName)\n",
    "    headers = \"title, title_date, subtitle_date, start_time, end_time, location, reg_course_name, reg_date, reg_start_time, reg_end_time, reg_location, reg_instructors\\n\"\n",
    "    f.write(headers)\n",
    "    logging.debug('wrote headers')\n",
    "    scrape(my_url, f)\n",
    "    f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Related reading material\n",
    "\n",
    "   - [urllib.request Documentation](https://docs.python.org/3/library/urllib.request.html#module-urllib.request)\n",
    "   - [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "   - [Python Web Scraping Tutorial using BeautifulSoup](https://www.dataquest.io/blog/web-scraping-tutorial-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
